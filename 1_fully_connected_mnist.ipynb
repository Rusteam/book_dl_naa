{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Hand-Written Digit Recognition on MNIST using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials import mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = mnist.input_data.read_data_sets('MNIST_data/', one_hot=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "hidden_units = [512,256,128]\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tf.placeholder(tf.float32, [None, input_size])\n",
    "y_data = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "def network(input_data, hidden_units, input_size, output_size, init_mean=0, init_std=0.1):\n",
    "    '''\n",
    "    Forward pass thru a fully connected network\n",
    "    '''\n",
    "    if isinstance(hidden_units, int):\n",
    "        weights_hidden = tf.Variable(tf.truncated_normal([input_size, hidden_units], mean=0, stddev=0.1))\n",
    "        biases_hidden = tf.Variable(tf.truncated_normal([hidden_units], mean=0, stddev=0.1))\n",
    "        hidden = tf.nn.relu(tf.matmul(x_data, weights_hidden) + biases_hidden)\n",
    "        hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "        weights_output = tf.Variable(tf.zeros([hidden_units, num_classes]))\n",
    "        biases_output = tf.Variable(tf.zeros([num_classes]))\n",
    "        logits = tf.nn.softmax(tf.matmul(hidden, weights_output) + biases_output)\n",
    "    elif isinstance(hidden_units, (list, tuple)):\n",
    "        hidden_units.insert(0, input_size)\n",
    "        for u_in,u_out in zip(hidden_units[:-1], hidden_units[1:]):\n",
    "            weights_hidden = tf.Variable(tf.truncated_normal([u_in, u_out], mean=init_mean, stddev=init_std))\n",
    "            biases_hidden = tf.Variable(tf.truncated_normal([u_out], mean=init_mean, stddev=init_std))\n",
    "            input_data = tf.matmul(input_data, weights_hidden) + biases_hidden\n",
    "            input_data = tf.nn.dropout(input_data, keep_prob)\n",
    "        weights_output = tf.Variable(tf.zeros([hidden_units[-1], num_classes]))\n",
    "        biases_output = tf.Variable(tf.zeros([num_classes]))\n",
    "        logits = tf.nn.softmax(tf.matmul(input_data, weights_output) + biases_output)            \n",
    "    else:\n",
    "        raise Exception('Hidden units argument must be int or list/tuple')\n",
    "    return logits\n",
    "    \n",
    "logits = network(x_data, hidden_units, input_size, num_classes)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_data, logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(y_data, 1), tf.argmax(logits, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'step': [],\n",
    "    'score': [],\n",
    "    'val_score': [],\n",
    "    'loss': [],\n",
    "    'val_loss': [],\n",
    "}\n",
    "\n",
    "for i in range(steps):\n",
    "    metrics['step'].append(i)\n",
    "    x_batch, y_batch = data.train.next_batch(batch_size)\n",
    "    l,_ = sess.run([loss, optimizer], \n",
    "                   feed_dict={x_data: x_batch, y_data: y_batch, keep_prob: dropout})\n",
    "    acc = sess.run(accuracy, \n",
    "                   feed_dict={x_data: x_batch, y_data: y_batch, keep_prob: 1.})\n",
    "    metrics['score'].append(acc), metrics['loss'].append(l)\n",
    "    val_l, val_acc = sess.run([loss, accuracy], \n",
    "                              feed_dict={x_data: data.validation.images, y_data: data.validation.labels, keep_prob: 1.})\n",
    "    metrics['val_score'].append(val_acc), metrics['val_loss'].append(val_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = sess.run(accuracy, feed_dict={x_data: data.test.images, y_data: data.test.labels, keep_prob: 1.})\n",
    "\n",
    "print('Accuracy after %d steps: %s' % (steps, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(metrics, fig_shape=(12,4)):\n",
    "    '''\n",
    "    Plots learning curve for loss and score both for train and validation\n",
    "    -------\n",
    "    :metrics is a dict with following keys: ['step', 'val_loss', 'loss', 'val_score', 'score'], where values are lists\n",
    "    '''\n",
    "    plt.figure(figsize=fig_shape)\n",
    "    plt.subplot(121)\n",
    "    plt.title('Loss curves')\n",
    "    plt.plot(metrics['step'], metrics['loss'], 'r--', label='train loss')\n",
    "    plt.plot(metrics['step'], metrics['val_loss'], 'b--', label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.title('Score curves')\n",
    "    plt.plot(metrics['step'], metrics['score'], 'r--', label='train score')\n",
    "    plt.plot(metrics['step'], metrics['val_score'], 'b--', label='validation score')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture report\n",
    "\n",
    "|steps|hidden units|accuracy|\n",
    "|---|---|---|\n",
    "|10000|0|0.92|\n",
    "|10000|[100]|0.962|\n",
    "|10000|[100,50]|0.88|\n",
    "|10000|[256,128]|0.8915|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
